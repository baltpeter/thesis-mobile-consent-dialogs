# Related Work

A lot of research has been done on data protection in websites and apps. We focus this overview on research on consent dialogs in the wild, their design (especially with regards to dark patterns and nudging), and detecting privacy violations through traffic analysis. Interest in these topics has resurged after the GDPR came into force in 2018, with much of recent research using the GDPR as a basis for investigations.

## Analysis of Consent Dialogs and Privacy Policies

Prior research into consent dialogs in the wild has so far been almost exclusively limited to the web. Degeling et al. [@degelingWeValueYour2019] monitored changes in privacy practices on 6.5k websites between December 2017 and October 2018 to measure the GDPR's impact. They found 16% of websites adding new privacy polices and 73% of websites updating their their existing ones. They further found an increase in consent dialogs being shown on websites by 16%. In a follow-up study the same year [@utzInformedConsentStudying2019a], they manually looked at screenshots of 1000 consent dialogs on websites and identified eight variables in which they differ, including size and position.

Still in 2019, Eijk et al. [@eijkImpactUserLocation2019a] detected consent dialogs in websites based on an adblock filter list, finding 40% of sites having a cookie dialog or notice. They discovered differences in cookie dialog prevalence between different countries (including ones in the EU), with those differences mostly being based on the sites' instead of the users' location. In 2020, Mehrnezhad manually analysed 116 websites and their corresponding mobile apps, finding consent dialogs in 91% of websites but only in 35% of apps.

Touching on the legal aspects, a 2020 paper by Matte et al. [@mattePurposesIABEurope2020a] studied the purposes of the IAB TCF and their usage by advertisers. They found that several of the purpose descriptions were not specific enough and many advertisers relied on a questionable legitimate interest, thus likely lacking a legal basis for their processing. The same year, Matte et al. also analysed websites using consent dialogs following the TCF [@matteCookieBannersRespect2020]. They detected violations like consent being registered without user interaction or even after opt-out, and preselected options in half of the sites. Similarly, Nouwens et al. also found common dark patterns, namely preticked checkboxes, implicitly assumed consent, and rejecting the dialog being harder than accepting it, by scraping 680 from the top 10k sites in the UK for their consent dialog designs using CMP-specific adapters for the five most common ones.

A 2021 thesis by Aerts [@aertsCookieDialogsTheir2021] detected consent dialogs on EU websites also based on filter lists and the TCF. They detected a vast majority of sites either not offering a a first-level "reject" button at all or highlighting the "accept" button. In May 2021, consumer protection organisation noyb analysed popular websites using the OneTrust CMP and found violations in more than 500 sites, with the most common ones being sites making it harder to refuse and withdraw consent than giving it and highlighting the "accept" over the "reject" button [@noyb.euNoybAimsEnd2021].

For privacy policy analysis, Harkous et al. [@harkousPolisisAutomatedAnalysis2018] and Hossein et al. [@hosseiniUnifyingPrivacyPolicy2021] propose machine-learning approaches. Zimmeck et al. specifically analysed the privacy policies of Android apps and compared them against the apps' permissions, API usage, and library inclusion [@zimmeckMAPSScalingPrivacy2019].

## Consent Dialog Design and Dark Patterns

Consent fatigue and the effect of nudging on consent rates are well established in relevant literature: Users will often use the easiest choice to dismiss a consent dialog as quickly as possible, without thinking about what it entails. Companies commonly exploit this by highlighting the most privacy-invasive choice and making it harder to choose anything else.

A 2010 experiment by Böhme et al. [@bohmeTrainedAcceptField2010a] already found that users are trained to accept EULAs and tend to blindly accept anything that resembles them. They further found that button labels had the largest effect on consent rates, with button labels indicating an actual choice resulting in much fewer opt-ins. A 2013 experiment by Bauer et al. [@bauerComparisonUsersPerceptions2013] investigated users' perception of and willingness to use "log in with \<social network>" functionality, also looking at the dialogs that prompt for consent to forward the user's information to the requesting website. They confirmed that consent dialogs don't work as a medium for conveying privacy-critical information to users, even when those users are concerned about their privacy. And in 2016, Bösch et al. [@boschTalesDarkSide2016] introduced a series of dark patterns common in websites and apps to deceive users into agreeing to more privacy-invasive practices than they actually want.

There is also already a plethora of work on the human aspects on consent dialogs specifically under the GDPR. Experiments by both Utz et al. in 2019 [@utzInformedConsentStudying2019a] and Nouwens et al. in 2020 [@nouwensDarkPatternsGDPR2020] investigated the effect of various design variables in consent dialogs on user choices and found that nudging, even in the form of seemingly small details, heavily increases consent rates.

Research on this topic also frequently discusses the ethical aspects of consent dialogs. A 2020 experiment by Machuletz et al. [@machuletzMultiplePurposesMultiple2020a] that explored users' interactions with consent dialogs again found that a highlighted "accept all" button results in significantly higher consent rates but at the same time users are not aware of its effects and regret their choice after being informed of them. Based on that, they call the morality and legitimacy of "accept all" buttons into question. And a 2021 experiment by Graßl et al. [@grasslDarkBrightPatterns2021] found most participants agreeing to all consent requests regardless of dark patterns. They hypothesise that nudging and dark patterns in consent dialogs have for a long time been so common that people became conditioned to them and their behaviour is influenced even in the absence of nudges. Conversely, they found that "bright patterns", i.e. nudging towards the privacy-friendly option, was effective in making people choose that.

Fassl et al. compiled a literature review in 2021 [@fasslStopConsentTheater2021], also looking at potential solutions to the discovered problems. Still in the same year, Gray et al. [@grayDarkPatternsLegal2021] presented a transdisciplinary *interaction criticism* of dark patterns in consent dialogs, providing arguments for further policy refinement and advancement.

## Traffic Analysis of Websites and Apps

Finally, we look at research that analyses the traffic of websites and apps to detect privacy violations. The GDPR coming into force has resulted in a decrease in cookie and third-party tracking use, to a certain degree also profiting people in non-EU countries, as reported by Hu et al. and Dabrowski et al. in 2019 [@huCharacterisingThirdParty2019a; @dabrowskiMeasuringCookiesWeb2019]. Nonetheless, tracking is still common. Also in 2019, Trevisan et al. introduced CookieCheck [@trevisanYearsEUCookie2019a], a tool that visits websites, doesn't provide consent, and checks whether tracking cookies (as classified by Ghostery [@ghosterygmbhWeMakePrivacy2022] and Disconnect [@disconnectinc.TrackingProtectionLists2015]) have been set. They found that 74% of websites still use third-party cookies. Increasingly, websites also use "post-cookie" tracking techniques like fingerprinting and tracking ID synchronisation. A 2021 study by Papadogiannakis et al. [@papadogiannakisUserTrackingPostcookie2021b] found websites using those with 75% of the detected tracking happening before the user interacted with a consent prompt or even after explicitly rejecting it.

Looking at the Android ecosystem, a 2018 study by Ren et al. [@renBugFixesImprovements2018] compared various versions of 512 Android apps across eight years, finding increased collection of personal data over time. In 2020, Liu et al. implemented MadDroid, a framework for automatically detecting malicious ad content in Android apps and found 6% of apps showing malicious ads. And a 2021 paper by Nguyen et al. [@nguyenShareFirstAsk2021] presented an analysis of the network traffic of 86k Android apps with no interaction. They classified the contacted domains by whether they belong to third-party trackers and found that 25k apps contacted trackers without consent. They also performed a survey among the violating app developers, finding common misconceptions about controllers' obligations under the GDPR.  
The Exodus Privacy project [@exoduscontributorsWhatExodusPrivacy2020] has collected a list of tracker signatures, using static analysis to check for their presence in Android apps. The TrackerControl app [@kollnigTrackerControlAndroid2022] uses a local VPN, allowing Android users an insight into which domains their apps contact.

Meanwhile on iOS, until recently, research into privacy violations by apps, has been scarce and outdated. A 2011 paper by Egele et al. [@egelePiOSDetectingPrivacy2011a] explored using static analysis on app binaries to detect privacy leaks and a 2015 paper by Dehling et al. [@dehlingExploringFarSide2015a] crawled app store pages of health apps.  
This changed in 2021 with AppChk by Geier et al. [@geierAppChkCrowdSourcingPlatform2021], an iOS app that also utilises a local VPN to them to collect the domains contacted by apps. Starting with, iOS 15.2 Apple has since integrated the *App Privacy Report*, a similar feature into the operating system itself [@appleinc.AppPrivacyReport2021]. Most recently, a 2022 study by Kollnig et al. [@kollnigAreIPhonesReally2022] compared the privacy practices of 12k apps each on Android and iOS. They found widespread violations against applicable privacy regulation, with little difference between the platforms.
